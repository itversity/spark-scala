{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark Write APIs\n",
    "\n",
    "Let us understand how we can write Data Frames to different file formats.\n",
    "* All the batch write APIs are grouped under write which is exposed to Data Frame objects.\n",
    "* All APIs are exposed under spark.read\n",
    "  * `text` - to write single column data to text files.\n",
    "  * `csv` - to write to text files with delimiters. Default is a comma, but we can use other delimiters as well.\n",
    "  * `json` - to write data to JSON files\n",
    "  * `orc` - to write data to ORC files\n",
    "  * `parquet` - to write data to Parquet files.\n",
    "* We can also write data to other file formats by plugging in and by using `write.format`, for example **avro**\n",
    "* We can use options based on the type using which we are writing the Data Frame to.\n",
    "  * `compression` - Compression codec (`gzip`, `snappy` etc)\n",
    "  * `sep` - to specify delimiters while writing into text files using **csv**\n",
    "* We can `overwrite` the directories or `append` to existing directories using `mode`\n",
    "* Create copy of orders data in **parquet** file format with no compression. If the folder already exists overwrite it. Target Location: **/user/[YOUR_USER_NAME]/retail_db/orders**\n",
    "* When you pass options, if there are typos then options will be ignored rather than failing. Be careful and make sure that output is validated.\n",
    "* By default the number of files in the output directory is equal to number of tasks that are used to process the data in the last stage. However, we might want to control number of files so that we don\"t run into too many small files issue.\n",
    "* We can control number of files by using `coalesce`. It has to be invoked on top of Data Frame before invoking `write`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"\"\"order_id INT, \n",
    "              order_date STRING, \n",
    "              order_customer_id INT, \n",
    "              order_status STRING\n",
    "           \"\"\"\n",
    "          ).\n",
    "    csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val username = System.getProperty(\"user.name\")\n",
    "\n",
    "orders.\n",
    "    write.\n",
    "    mode(\"overwrite\").\n",
    "    option(\"compression\", \"none\").\n",
    "    parquet(s\"/user/${username}/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Alternative approach - using format\n",
    "val username = System.getProperty(\"user.name\")\n",
    "\n",
    "orders.\n",
    "    write.\n",
    "    mode(\"overwrite\").\n",
    "    option(\"compression\", \"none\").\n",
    "    format(\"parquet\").\n",
    "    save(s\"/user/${username}/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "\n",
    "s\"hdfs dfs -ls /user/${username}/retail_db/orders\" !\n",
    "\n",
    "// File extension should not contain compression algorithms such as snappy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read order_items data from **/public/retail_db_json/order_items** and write it to pipe delimited files with gzip compression. Target Location: **/user/[YOUR_USER_NAME]/retail_db/order_items**. Make sure to validate.\n",
    "* Ignore the error if the target location already exists. Also make sure to write into only one file. We can use `coalesce` for it. \n",
    "\n",
    "**`coalesce` will be covered in detail at a later point in time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val order_items = spark.\n",
    "    read.\n",
    "    json(\"/public/retail_db_json/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using format\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "\n",
    "order_items.\n",
    "    coalesce(1).\n",
    "    write.\n",
    "    mode(\"ignore\").\n",
    "    option(\"compression\", \"gzip\").\n",
    "    option(\"sep\", \"|\").\n",
    "    format(\"csv\").\n",
    "    save(s\"/user/${username}/retail_db/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "\n",
    "s\"hdfs dfs -ls /user/${username}/retail_db/order_items\" !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
