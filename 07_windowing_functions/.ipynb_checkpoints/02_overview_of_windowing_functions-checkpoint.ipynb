{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Windowing Functions\n",
    "\n",
    "Let us get an overview of Windowing Functions.\n",
    "\n",
    " * First let us understand relevance of these functions using `employees` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided.\n",
    "\n",
    "If you want to use terminal for the practice, here is the command to use.\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "  --master yarn \\\n",
    "  --name \"Joining Data Sets\" \\\n",
    "  --conf spark.ui.port=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Windowing Functions\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val employeesPath = \"/public/hr_db/employees\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val employees = spark.\n",
    "    read.\n",
    "    format(\"csv\").\n",
    "    option(\"sep\", \"\\t\").\n",
    "    schema(\"\"\"employee_id INT, \n",
    "              first_name STRING, \n",
    "              last_name STRING, \n",
    "              email STRING,\n",
    "              phone_number STRING, \n",
    "              hire_date STRING, \n",
    "              job_id STRING, \n",
    "              salary FLOAT,\n",
    "              commission_pct STRING,\n",
    "              manager_id STRING, \n",
    "              department_id STRING\n",
    "            \"\"\").\n",
    "    load(employeesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.\n",
    "    select($\"employee_id\", \n",
    "           $\"department_id\".cast(\"int\").alias(\"department_id\"), \n",
    "           $\"salary\"\n",
    "          ).\n",
    "    orderBy(\"department_id\", \"salary\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us say we want to compare individual salary with department wise salary expense.\n",
    "* Here is one of the approach which require self join.\n",
    "  * Compute department wise expense usig `groupBy` and `agg`.\n",
    "  * Join with **employees** again on department_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{sum, col}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val department_expense = employees.\n",
    "    groupBy(\"department_id\").\n",
    "    agg(sum(\"salary\").alias(\"expense\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_expense.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.\n",
    "    select(\"employee_id\", \"department_id\", \"salary\").\n",
    "    join(department_expense, employees(\"department_id\") === department_expense(\"department_id\")).\n",
    "    orderBy(employees(\"department_id\"), $\"salary\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **However, using this approach is not very efficient and also overly complicated. Windowing functions actually simplify the logic and also runs efficiently**\n",
    " \n",
    "Now let us get into the details related to Windowing functions.\n",
    " * Main package `org.apache.spark.sql.expressions`\n",
    " * It has classes such as `Window` and `WindowSpec`\n",
    " * `Window` have APIs such as `partitionBy`, `orderBy` etc\n",
    " * These APIs (such as `partitionBy`) return `WindowSpec` object. We can pass `WindowSpec` object to over on functions such as `rank()`, `dense_rank()`, `sum()` etc\n",
    " * Syntax: `sum().over(spec)` where `spec = Window.partitionBy(\"ColumnName\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Functions        | API or Function      |\n",
    "| ------------- |:-------------:|\n",
    "| Aggregate Functions      | <ul><li>sum</li><li>avg</li><li>min</li><li>max</li></ul> |\n",
    "| Ranking Functions      | <ul><li>rank</li><li>dense_rank</li></ul><ul><li>percent_rank</li><li>row_number</li> <li>ntile</li></ul> |\n",
    "| Analytic Functions      | <ul><li>cume_dist</li><li>first</li><li>last</li><li>lead</li> <li>lag</li></ul> |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
