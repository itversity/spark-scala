{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Functions\n",
    "\n",
    "We can use ranking functions to assign ranks to a particular record within a partition.\n",
    "\n",
    "* Sparse Rank - rank\n",
    "* Dense Rank - dense_rank\n",
    "* Assigning Row Numbers - row_number\n",
    "* Percentage Rank - percent_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided.\n",
    "\n",
    "If you want to use terminal for the practice, here is the command to use.\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "  --master yarn \\\n",
    "  --name \"Joining Data Sets\" \\\n",
    "  --conf spark.ui.port=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Windowing Functions\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform few tasks related to ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airlines_path = \"/public/airlines_all/airlines-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val airlines = spark.\n",
    "    read.\n",
    "    parquet(airlines_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, lit, lpad, concat}\n",
    "import org.apache.spark.sql.functions.{rank, dense_rank}\n",
    "import org.apache.spark.sql.functions.{percent_rank, row_number, round}\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spec = Window.\n",
    "    partitionBy(\"FlightDate\", \"Origin\").\n",
    "    orderBy(col(\"DepDelay\").desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.\n",
    "    filter(\"IsDepDelayed = 'YES' and Cancelled = 0\").\n",
    "    select(concat($\"Year\", \n",
    "                  lpad($\"Month\", 2, \"0\"), \n",
    "                  lpad($\"DayOfMonth\", 2, \"0\")\n",
    "                 ).alias(\"FlightDate\"),\n",
    "           $\"Origin\",\n",
    "           $\"UniqueCarrier\",\n",
    "           $\"FlightNum\",\n",
    "           $\"CRSDepTime\",\n",
    "           $\"IsDepDelayed\",\n",
    "           $\"DepDelay\".cast(\"int\").alias(\"DepDelay\")\n",
    "          ).\n",
    "    withColumn(\"srank\", rank().over(spec)).\n",
    "    withColumn(\"drank\", dense_rank().over(spec)).\n",
    "    withColumn(\"prank\", round(percent_rank().over(spec), 2)).\n",
    "    withColumn(\"rn\", row_number().over(spec)).\n",
    "    orderBy($\"FlightDate\", $\"Origin\", $\"DepDelay\".desc).\n",
    "    show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
