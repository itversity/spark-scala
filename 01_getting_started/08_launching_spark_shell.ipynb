{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting File Metadata\n",
    "\n",
    "Let us review the source location to get number of files and the size of the data we are going to process.\n",
    "\n",
    "* Location of airlines data dbfs:/databricks-datasets/airlines\n",
    "* We can get first 1000 files using %fs ls dbfs:/databricks-datasets/airlines\n",
    "* Location contain 1919 Files, however we will not be able to see all the details using %fs command.\n",
    "* Databricks File System commands does not have capability to understand metadata of files such as size in details.\n",
    "* When Spark Cluster is started, it will create 2 objects - spark and sc\n",
    "* sc is of type SparkContext and spark is of type SparkSession\n",
    "* Spark uses HDFS APIs to interact with the file system and we can access HDFS APIs using sc._jsc and sc._jvm to get file metadata.\n",
    "* Here are the steps to get the file metadata. \n",
    "  * Get Hadoop Configuration using ` sc._jsc.hadoopConfiguration()` - let\"s say `conf`\n",
    "  * We can pass conf to ` sc._jvm.org.apache.hadoop.fs.FileSystem.` get to get FileSystem object - let\"s say `fs`\n",
    "  * We can build ` path`  object by passing the path as string to `sc._jvm.org.apache.hadoop.fs.Path`\n",
    "  * We can invoke `listStatus` on top of fs by passing path which will return an array of FileStatus objects - let\"s say files.  \n",
    "  * Each `FileStatus` object have all the metadata of each file.\n",
    "  * We can use `len` on files to get number of files.\n",
    "  * We can use `getLen` on each `FileStatus` object to get the size of each file. \n",
    "  * Cumulative size of all files can be achieved using `sum(map(lambda file: file.getLen(), files))` \n",
    "* Let us first get list of files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls dbfs:/databricks-datasets/airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the consolidated script to get number of files and cumulative size of all files in a given folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@6a105b5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@6a105b5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    appName(\"Getting Started\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = spark.sparkContext.hadoopConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1742269756_44, ugi=training (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1742269756_44, ugi=training (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.fs.FileSystem\n",
    "val fs = FileSystem.get(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path = /public/airlines_all/airlines\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "/public/airlines_all/airlines"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.fs.Path\n",
    "val path = new Path(\"/public/airlines_all/airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "files = Array(FileStatus{path=hdfs://nn01.itversity.com:8020/public/airlines_all/airlines/README.md; isDirectory=false; length=1089; replication=2; blocksize=134217728; modification_time=1572112932387; access_time=1584372852270; owner=hdfs; group=hdfs; permission=rw-r--r--; isSymlink=false}, FileStatus{path=hdfs://nn01.itversity.com:8020/public/airlines_all/airlines/_SUCCESS; isDirectory=false; length=0; replication=2; blocksize=134217728; modification_time=1572112932485; access_time=1580753965620; owner=hdfs; group=hdfs; permission=rw-r--r--; isSymlink=false}, FileStatus{path=hdfs://nn01.itversity.com:8020/public/airlines_all/airlines/part-00000; isDirectory=false; length=67108879; replication=2; blocksize=134217728; modification_time=15721129332...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(FileStatus{path=hdfs://nn01.itversity.com:8020/public/airlines_all/airlines/README.md; isDirectory=false; length=1089; replication=2; blocksize=134217728; modification_time=1572112932387; access_time=1584372852270; owner=hdfs; group=hdfs; permission=rw-r--r--; isSymlink=false}, FileStatus{path=hdfs://nn01.itversity.com:8020/public/airlines_all/airlines/_SUCCESS; isDirectory=false; length=0; replication=2; blocksize=134217728; modification_time=1572112932485; access_time=1580753965620; owner=hdfs; group=hdfs; permission=rw-r--r--; isSymlink=false}, FileStatus{path=hdfs://nn01.itversity.com:8020/public/airlines_all/airlines/part-00000; isDirectory=false; length=67108879; replication=2; blocksize=134217728; modification_time=15721129332..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val files = fs.listStatus(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.map(file => file.getLen).sum/1024/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
