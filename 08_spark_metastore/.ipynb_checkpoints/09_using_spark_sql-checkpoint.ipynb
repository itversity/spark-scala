{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark SQL\n",
    "\n",
    "Let us understand how we can use Spark SQL to process data in Metastore Tables and Temporary Views.\n",
    "\n",
    "* Once tables are created in metastore or temporary views are created, we can run queries against the tables to perform all standard transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided.\n",
    "\n",
    "If you want to use terminal for the practice, here is the command to use.\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "  --master yarn \\\n",
    "  --name \"Joining Data Sets\" \\\n",
    "  --conf spark.ui.port=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Spark Metastore\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_airlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here are some of the transformations which can be performed.\n",
    "  * Row Level Transformations using functions in SELECT clause.\n",
    "  * Filtering using WHERE clause\n",
    "   * Aggregations using GROUP BY and aggregate functions.\n",
    "  * Sorting using ORDER BY or SORT BY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform some tasks to understand how to process data using Spark SQL using Metastore Tables or Temporary Views.\n",
    "* Make sure table or view created for airport-codes. We can use the table or view created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a query to get number of airports per state in the US. \n",
    " * Get only those states which have more than 10 airports.\n",
    " * Make sure data is sorted in descending order by number of airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.\n",
    "    sql(\"\"\"SELECT state, count(1) AS airport_cnt\n",
    "           FROM airport_codes\n",
    "           GROUP BY state\n",
    "               HAVING count(1) >= 10\n",
    "           ORDER BY airport_cnt DESC\n",
    "        \"\"\").\n",
    "  show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
